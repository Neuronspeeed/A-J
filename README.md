# Latent Thinking Experiment

Test whether AI models can "think" about a problem while appearing to work on something completely unrelated. Hidden cognitive processes can be captured and transferred between AI sessions?

## What is "Latent Thinking"?

**Latent thinking** refers to the hypothesis that AI models might engage in hidden cognitive processes while working on one task that could influence their performance on a subsequent task. The experiment tests whether AI models can "think" about a problem while appearing to work on something completely unrelated.

### Experimental Outcomes:
- **Phase 1 Hypothesis Supported**: Models demonstrate improved mathematical reasoning when engaging with preliminary tasks (89.4% improvement, p = 0.0158)
- **Phase 2 Hypothesis Refuted**: Numerical patterns do not transfer semantic "thinking" between sessions; improvement derives from attention mechanism activation
- **Mechanistic Discovery**: Numerical tokens activate mathematical reasoning circuits through transformer attention mechanisms, independent of semantic content

## The Three-Phase Experimental Design

### **Phase 1: "Do models think while talking about something unrelated?"**

**Core Question**: Does engaging with unrelated text before solving a math problem affect the model's accuracy?

**How it works**:
- Models are given **two questions** in a single prompt using XML tags
- **Question 1**: A "filler" task (varies by condition)
- **Question 2**: A challenging math word problem
- The model must answer both using `<answer1>` and `<answer2>` tags

**The 6 Conditions Tested**:

1. **BASELINE** (control condition)
   - "Is the sky blue? yes/no" followed by math problem
   - Establishes baseline mathematical reasoning performance
   - Mean accuracy: 6.400 digits correct

2. **THINK_ABOUT_SOLUTION** (metacognitive priming)  
   - "Think carefully about how you would solve the second question"
   - Tests effect of explicit problem consideration
   - Mean accuracy: 12.123 digits correct (89.4% improvement)

3. **MEMORIZED**
   - "Sing Happy Birthday" followed by math problem
   - Tests low cognitive load retrieval task impact
   - Mean accuracy: 8.417 digits correct (31.5% improvement)

4. **COMPLEX_STORY**
   - "Write a complex story in about 150 words" followed by math problem  
   - Tests cognitive resource competition effects
   - Mean accuracy: 6.915 digits correct (8.1% improvement)

5. **PYTHON_PROGRAM**
   - "Write a Python program that prints Fibonacci numbers" followed by math problem
   - Tests logical/numerical task priming effects
   - Mean accuracy: 8.400 digits correct (31.2% improvement)

6. **GENERATE_RANDOM_NUMBERS** (data collection for Phase 2)
   - "Make a bunch of random numbers" followed by math problem
   - Generates numerical sequences for Phase 2 transplantation testing
   - Mean accuracy: 9.018 digits correct (40.9% improvement)
   - Note: Excluded from Phase 1 experimental analysis

**Math Problems**: 10 challenging word problems requiring multi-step reasoning:
- Train overtaking problems 
- Investment problems
- Boat/current problems 
- Exponential growth/decay
- Logarithmic equations
- Trigonometric equations

### **Phase 2: "Can you transplant thinking and have it still work?"**

**Core Question**: Can "thinking" patterns be transmitted through random numbers?

**The Hypothesis**: If Phase 1 shows that generating random numbers improves math performance, can those same AI-generated numbers improve performance when provided to a fresh model session?

**How it works**:
- Takes the random numbers generated by AI models in Phase 1
- Provides them to models in a new session as "helpful context"
- Tests if this "transplanted thinking" improves accuracy

**The 3 Conditions**:

1. **BASELINE_NO_NUMBERS**
   - Pure math problem, no additional context
   - Control condition

2. **WITH_TRANSPLANTED_NUMBERS**  
   - "You will be asked a question. But first, here are a bunch of text that might help you: [AI-generated numbers from Phase 1]"
   - Tests the core transplant hypothesis

3. **WITH_RANDOM_NUMBERS**
   - "You will be asked a question. But first, here are a bunch of text that might help you: [completely random numbers]"
   - Controls for whether any numbers help vs. specifically AI-generated ones

### **Phase 3: Cross-Problem Transplantation** (Planned/Future)

**Core Question**: Can thinking patterns transfer across different types of problems?

**Hypothesis**: If transplantation works within the same problem, can numbers generated while solving Problem A help with Problem B?


## Experimental Methodology

**Accuracy Measurement**: 
- Counts correct digits from the start of numerical answers
- Avoids binary right/wrong to capture partial understanding
- Example: Expected "8.7804", Got "8.7823" → 3 digits correct

**Statistical Power**:
- Multiple iterations per condition (3+ runs)
- Multiple models tested (GPT-4 family + Claude-4 family)  
- Exponential backoff for API reliability

**Experimental Controls**:
- Identical system prompts within phases
- Same mathematical problems across conditions
- Randomized number generation with reproducible seeds

## Expected Outcomes & Implications

**Phase 1 Predictions**:
- BASELINE: Lowest accuracy (no cognitive warm-up)
- THINK_ABOUT_SOLUTION: Highest accuracy (direct preparation)
- Others: Intermediate, possibly showing unexpected patterns

**Phase 2 Predictions**:
- If transplantation works: Transplanted > Baseline > Random numbers
- If it fails: All conditions perform similarly
- Breakthrough result: AI models encode meaningful patterns in seemingly random output

**Implications**:
- Could reveal hidden structure in AI reasoning
- Might enable new techniques for AI performance enhancement
- Tests fundamental questions about machine cognition

**Research Question**: "Do AI models have a hidden 'mental state' that can be captured and reused?"

## How the Code is Organized

-   **`core/`**: Contains the project's main components:
    -   `data_models.py`: Defines the data structures for the project (e.g., `TrialResult`, `ExperimentConfig`).
    -   `llm_providers.py`: Contains the code for interacting with different LLM providers like OpenAI and Anthropic.
    -   `persistence.py`: Contains the code for saving experiment results to files.
-   **`engine/`**: Contains `ExperimentRunner`, the main class that runs the experiment. It can work with any LLM provider or storage method defined in `core`.
-   **`config/`**: Defines the exact experimental conditions (prompts, problems, models) for each phase. This separates the experiment's configuration from its logic.
-   **`main_*.py`**: Scripts to start the experiments. They set up the necessary components and pass them to the main experiment runner.
-   **`tests/`**: Contains automated tests to verify the experiment logic works correctly without making real API calls.

## How to Run the Experiment

### Prerequisites

-   Python 3.11+
-   An active virtual environment (e.g., via `uv venv`)

### Installation

1.  Install dependencies:
    ```bash
    uv sync
    ```

2.  Set up API keys:
    ```bash
    # Copy the example file and edit it
    cp env.example .env
    
    # Edit .env with your actual API keys:
    # OPENAI_API_KEY=sk-your-openai-api-key-here
    # ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
    ```

### Running the Experiments

The two phases must be run in order.

**1. Run Phase 1 (Data Generation)**

This runs the filler-question experiment and generates the data needed for Phase 2.

```bash
python main_phase1.py
```

This will create a results file named `phase1_results_<timestamp>.csv`.

**2. Run Phase 2 (Transplant Thinking)**

This automatically finds the latest Phase 1 results, harvests the AI-generated numbers, and runs the transplant experiment.

```bash
python main_phase2.py
```

This will create a results file named `phase2_results_<timestamp>.csv`.

### Analyzing Results

After running experiments, use the analysis scripts to generate reports:

**Comprehensive Verification (Recommended):**
```bash
python analysis/comprehensive_verification.py
```
Generates complete verification with all calculations, data quality checks, and visualizations saved to `data/` folder.

**Individual Analysis:**
```bash
python analysis/generate_phase1_reports.py --summary-only
python analysis/generate_phase1_reports.py --visualize  # With charts
python analysis/generate_phase2_reports.py --summary-only
python analysis/generate_phase2_reports.py --visualize  # With charts
```

**Generated Visualizations:**
- `data/comprehensive_analysis.png` - Complete overview
- `data/phase1_detailed_analysis.png` - Phase 1 detailed charts
- `data/phase2_detailed_analysis.png` - Phase 2 detailed charts
- `data/model_comparison.png` - Model performance comparison

**Key Findings:**
- **Latent Thinking Effect**: 89.4% improvement with metacognitive priming (p = 0.0158)
- **Number Injection Effect**: 33-40% improvement with numerical context injection
- **System Prompt Impact**: Enhanced prompts with behavioral examples improve baseline performance by 100%
- **Data Collection**: 480 experimental trials completed across both phases

## Testing

To run the automated tests:

```bash
pytest
```

To run tests with coverage report:

```bash
pytest --cov=core --cov=engine --cov-report=term-missing
```

The test suite runs automatically on all code changes to ensure reliability and prevent regressions.

## Results

### Phase 1: Latent Thinking Effect

300 experimental trials completed testing whether unrelated tasks affect mathematical reasoning performance.

**Primary finding:** 89.4% improvement when models engage in metacognitive priming
- **Think About Solution condition:** 12.123 digits correct (mean)
- **Baseline condition:** 6.400 digits correct (mean)
- **Statistical analysis:** t(115) = 2.450, p = 0.0158, Cohen's d = 0.453

**Performance by condition:**
1. Think About Solution: 12.123 digits (+89.4%)
2. Memorized Content: 8.417 digits (+31.5%)
3. Python Programming: 8.400 digits (+31.2%)
4. Complex Story: 6.915 digits (+8.1%)
5. Baseline: 6.400 digits (reference)

**Analysis:** Metacognitive priming demonstrates statistically significant improvement in mathematical reasoning accuracy.

### Phase 2: Number Injection Effect

180 trials completed testing numerical context effects on mathematical reasoning.

**Findings:** Random numbers produced stronger effects than AI-generated numbers
- **Random Numbers:** 8.632 digits (+39.9% vs baseline)
- **AI-Generated Numbers:** 8.246 digits (+33.7% vs baseline)
- **Baseline (No Numbers):** 6.169 digits (reference)

**Model-specific performance:**
- Claude Sonnet 4: 89.0% improvement with numerical context
- Claude 4 Opus: 2.2% improvement with numerical context

**Analysis:** The improvement mechanism operates through attention-based activation of mathematical processing circuits rather than semantic pattern transfer. The effect is independent of number source or meaning.

### Verification and Analysis

**Complete verification with visualizations:**
```bash
python analysis/comprehensive_verification.py
```

**Individual analysis:**
```bash
python analysis/generate_phase1_reports.py --summary-only
python analysis/generate_phase2_reports.py --summary-only
```

**Generated files:** All visualizations saved to `data/` folder

## How to Extend the Experiment

The organized design makes extensions simple and safe.

-   **To Add a New Model (e.g., Gemini)**:
    1.  Add a `GeminiProvider` class in `core/llm_providers.py`.
    2.  Add its configuration to `config/experiments.py`.
    3.  Add the new model name to the `model_names` list in the desired experiment config.

-   **To Add a New Math Problem**:
    1.  Add a new `MathProblem` to the `MATH_PROBLEMS` list in `config/experiments.py`.

-   **To Add a New Condition**:
    1.  Add a new condition type in `core/data_models.py`.
    2.  Add a matching prompt template in `config/experiments.py`.
    3.  Add the new condition to the `conditions` list in the desired experiment config.

## Recent Updates

**Experimental Completion (August 5, 2025)**:
- Phase 1 and Phase 2 experiments completed with comprehensive data collection
- Identified and resolved configuration import bug affecting system prompt application
- Corrected baseline condition implementation to match experimental design
- Applied methodological corrections to exclude data harvesting trials from experimental analysis

**Technical Contributions**:
- Demonstrated statistically significant latent thinking effect (t = 2.450, p = 0.0158, d = 0.453)
- Identified attention-based mechanism for numerical context enhancement
- Quantified system prompt impact on baseline performance (2x improvement)
- Provided empirical evidence challenging thinking transplantation hypothesis

## Project Structure

```
.
├── analysis/
│   ├── generate_phase1_reports.py  # Phase 1 analysis and reports
│   ├── generate_phase2_reports.py  # Phase 2 analysis and reports
│   ├── analyze_phase2_results.py   # Legacy Phase 2 analysis
│   ├── ANALYSIS_USAGE.md           # Analysis usage guide
│   ├── PHASE2_RESULTS_REPORT.md    # Phase 2 results summary
│   └── README.md                   # Analysis documentation
├── config/
│   ├── experiments.py              # Phase 1 & 2 configurations
│   └── __init__.py
├── core/
│   ├── data_models.py              # Data structures
│   ├── llm_providers.py            # LLM API interfaces
│   ├── persistence.py              # Result storage
│   ├── data_manager.py             # Data utilities
│   ├── utils.py                    # Helper functions
│   └── __init__.py
├── data/
│   ├── phase1/                     # Phase 1 experimental results
│   ├── phase2/                     # Phase 2 experimental results
│   └── README.md                   # Data documentation
├── engine/
│   ├── experiment_runner.py        # Core experiment logic
│   └── __init__.py
├── old/                            # Original friend's experiments
├── tests/
│   ├── test_experiment_runner.py   # Engine tests
│   ├── test_core_modules.py        # Core module tests
│   └── __init__.py
├── main_phase1.py                  # Run Phase 1 experiment
├── main_phase2.py                  # Run Phase 2 experiment
├── pyproject.toml                  # Dependencies and config
├── REFACTOR_SUMMARY.md             # Code refactoring notes
├── TODAY_EXPERIMENT_WORK.md        # Recent experiment work
└── README.md                       # Project documentation

Analysis Commands:
  python analysis/generate_phase1_reports.py [--summary-only]
  python analysis/generate_phase2_reports.py [--summary-only]
```
