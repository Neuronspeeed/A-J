# Latent Thinking Experiment

Test whether AI models can "think" about a problem while appearing to work on something completely unrelated. Hidden cognitive processes can be captured and transferred between AI sessions?

## ðŸ§  What is "Latent Thinking"?

**Latent thinking** refers to the hypothesis that AI models might engage in hidden cognitive processes while working on one task that could influence their performance on a subsequent task. The experiment tests whether AI models can "think" about a problem while appearing to work on something completely unrelated.

##  The Three-Phase Experimental Design

### **Phase 1: "Do models think while talking about something unrelated?"**

**Core Question**: Does engaging with unrelated text before solving a math problem affect the model's accuracy?

**How it works**:
- Models are given **two questions** in a single prompt using XML tags
- **Question 1**: A "filler" task (varies by condition)
- **Question 2**: A challenging math word problem
- The model must answer both using `<answer1>` and `<answer2>` tags

**The 6 Conditions Tested**:

1. **BASELINE** (worst expected accuracy)
   - Math problem first, then "Is the sky blue? yes/no"
   - Tests pure math performance without cognitive warm-up

2. **THINK_ABOUT_SOLUTION** (best expected accuracy)  
   - "Think carefully about how you would solve the second question"
   - Direct meta-cognitive prompt about the math problem

3. **MEMORIZED**
   - "Sing Happy Birthday" + math problem
   - Tests if reciting memorized content affects performance

4. **COMPLEX_STORY**
   - "Write a complex story in about 150 words" + math problem  
   - Tests if creative writing affects mathematical reasoning

5. **PYTHON_PROGRAM**
   - "Write a Python program that prints Fibonacci numbers" + math problem
   - Tests if programming affects mathematical thinking

6. **GENERATE_RANDOM_NUMBERS** (crucial for Phase 2)
   - "Make a bunch of random numbers" + math problem
   - Harvests AI-generated numbers for transplant experiment

**Math Problems**: 10 challenging word problems requiring multi-step reasoning:
- Train overtaking problems 
- Investment problems
- Boat/current problems 
- Exponential growth/decay
- Logarithmic equations
- Trigonometric equations

### **Phase 2: "Can you transplant thinking and have it still work?"**

**Core Question**: Can "thinking" patterns be transmitted through random numbers?

**The Hypothesis**: If Phase 1 shows that generating random numbers improves math performance, can those same AI-generated numbers improve performance when provided to a fresh model session?

**How it works**:
- Takes the random numbers generated by AI models in Phase 1
- Provides them to models in a new session as "helpful context"
- Tests if this "transplanted thinking" improves accuracy

**The 3 Conditions**:

1. **BASELINE_NO_NUMBERS**
   - Pure math problem, no additional context
   - Control condition

2. **WITH_TRANSPLANTED_NUMBERS**  
   - "You will be asked a question. But first, here are a bunch of text that might help you: [AI-generated numbers from Phase 1]"
   - Tests the core transplant hypothesis

3. **WITH_RANDOM_NUMBERS**
   - "You will be asked a question. But first, here are a bunch of text that might help you: [completely random numbers]"
   - Controls for whether any numbers help vs. specifically AI-generated ones

### **Phase 3: Cross-Problem Transplantation** (Planned/Future)

**Core Question**: Can thinking patterns transfer across different types of problems?

**Hypothesis**: If transplantation works within the same problem, can numbers generated while solving Problem A help with Problem B?


## ðŸ”¬ 

**Accuracy Measurement**: 
- Counts correct digits from the start of numerical answers
- Avoids binary right/wrong to capture partial understanding
- Example: Expected "8.7804", Got "8.7823" â†’ 3 digits correct

**Statistical Power**:
- Multiple iterations per condition (3+ runs)
- Multiple models tested (GPT-4 family + Claude-4 family)  
- Exponential backoff for API reliability

**Experimental Controls**:
- Identical system prompts within phases
- Same mathematical problems across conditions
- Randomized number generation with reproducible seeds

##  Expected Outcomes & Implications

**Phase 1 Predictions**:
- BASELINE: Lowest accuracy (no cognitive warm-up)
- THINK_ABOUT_SOLUTION: Highest accuracy (direct preparation)
- Others: Intermediate, possibly showing unexpected patterns

**Phase 2 Predictions**:
- If transplantation works: Transplanted > Baseline > Random numbers
- If it fails: All conditions perform similarly
- Breakthrough result: AI models encode meaningful patterns in seemingly random output

**Implications**:
- Could reveal hidden structure in AI reasoning
- Might enable new techniques for AI performance enhancement
- Tests fundamental questions about machine cognition

 **"Do AI models have a hidden 'mental state' that can be captured and reused?"**

## How the Code is Organized

-   **`core/`**: Contains the project's main components:
    -   `data_models.py`: Defines the data structures for the project (e.g., `TrialResult`, `ExperimentConfig`).
    -   `llm_providers.py`: Contains the code for interacting with different LLM providers like OpenAI and Anthropic.
    -   `persistence.py`: Contains the code for saving experiment results to files.
-   **`engine/`**: Contains `ExperimentRunner`, the main class that runs the experiment. It can work with any LLM provider or storage method defined in `core`.
-   **`config/`**: Defines the exact experimental conditions (prompts, problems, models) for each phase. This separates the experiment's configuration from its logic.
-   **`main_*.py`**: Scripts to start the experiments. They set up the necessary components and pass them to the main experiment runner.
-   **`tests/`**: Contains automated tests to verify the experiment logic works correctly without making real API calls.

## How to Run the Experiment

### Prerequisites

-   Python 3.11+
-   An active virtual environment (e.g., via `uv venv`)

### Installation

1.  Install dependencies:
    ```bash
    uv sync
    ```

2.  Set up API keys:
    ```bash
    # Copy the example file and edit it
    cp env.example .env
    
    # Edit .env with your actual API keys:
    # OPENAI_API_KEY=sk-your-openai-api-key-here
    # ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
    ```

### Running the Experiments

The two phases must be run in order.

**1. Run Phase 1 (Data Generation)**

This runs the filler-question experiment and generates the data needed for Phase 2.

```bash
python main_phase1.py
```

This will create a results file named `phase1_results_<timestamp>.csv`.

**2. Run Phase 2 (Transplant Thinking)**

This automatically finds the latest Phase 1 results, harvests the AI-generated numbers, and runs the transplant experiment.

```bash
python main_phase2.py
```

This will create a results file named `phase2_results_<timestamp>.csv`.

### Analyzing Results

After running experiments, use the analysis scripts to generate reports:

**Comprehensive Verification (Recommended):**
```bash
python analysis/comprehensive_verification.py
```
Generates complete verification with all calculations, data quality checks, and visualizations saved to `data/` folder.

**Individual Analysis:**
```bash
python analysis/generate_phase1_reports.py --summary-only
python analysis/generate_phase1_reports.py --visualize  # With charts
python analysis/generate_phase2_reports.py --summary-only
python analysis/generate_phase2_reports.py --visualize  # With charts
```

**Generated Visualizations:**
- `data/comprehensive_analysis.png` - Complete overview
- `data/phase1_detailed_analysis.png` - Phase 1 detailed charts
- `data/phase2_detailed_analysis.png` - Phase 2 detailed charts
- `data/model_comparison.png` - Model performance comparison

**Key Verified Results:**
- **Phase 1**: +24.9% improvement when models "think about solution"
- **Phase 2**: +1.1% overall, +61.3% for gpt-4o-mini, +45.3% for gpt-4.1-mini
- **Data Quality**: 98.4% Phase 1 completion, 96.2% Phase 2 completion

## Testing

To run the automated tests:

```bash
pytest
```

To run tests with coverage report:

```bash
pytest --cov=core --cov=engine --cov-report=term-missing
```

The test suite runs automatically on all code changes to ensure reliability and prevent regressions.

## Results

### Phase 1: Thinking While Distracted

1,346 valid trials (98.4% completion) tested whether unrelated tasks affect math performance.

**Core finding:** +24.9% improvement when asked to "think about solution"
**Best condition:** memorized (4.261 digits correct)
**Baseline:** 3.180 digits correct

**Conclusion:** Models do think about problems while appearing to work on unrelated tasks.

### Phase 2: Thinking Transplant

606 valid trials (96.2% completion) tested whether AI-generated numbers improve performance.

**Overall:** +1.1% improvement with transplanted numbers
**Best performers:** gpt-4o-mini (+61.3%), gpt-4.1-mini (+45.3%)
**Interference effects:** gpt-4.1 (-50.0%), gpt-4.1-nano (-19.5%)

**Conclusion:** Effect is model-dependent. Smaller models benefit, larger models show interference.

### Verification and Analysis

**Complete verification with visualizations:**
```bash
python analysis/comprehensive_verification.py
```

**Individual analysis:**
```bash
python analysis/generate_phase1_reports.py --summary-only
python analysis/generate_phase2_reports.py --summary-only
```

**Generated files:** All visualizations saved to `data/` folder

## How to Extend the Experiment

The organized design makes extensions simple and safe.

-   **To Add a New Model (e.g., Gemini)**:
    1.  Add a `GeminiProvider` class in `core/llm_providers.py`.
    2.  Add its configuration to `config/experiments.py`.
    3.  Add the new model name to the `model_names` list in the desired experiment config.

-   **To Add a New Math Problem**:
    1.  Add a new `MathProblem` to the `MATH_PROBLEMS` list in `config/experiments.py`.

-   **To Add a New Condition**:
    1.  Add a new condition type in `core/data_models.py`.
    2.  Add a matching prompt template in `config/experiments.py`.
    3.  Add the new condition to the `conditions` list in the desired experiment config.

## Project Structure

```
.
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ generate_phase1_reports.py  # Phase 1 analysis and reports
â”‚   â”œâ”€â”€ generate_phase2_reports.py  # Phase 2 analysis and reports
â”‚   â”œâ”€â”€ analyze_phase2_results.py   # Legacy Phase 2 analysis
â”‚   â”œâ”€â”€ ANALYSIS_USAGE.md           # Analysis usage guide
â”‚   â”œâ”€â”€ PHASE2_RESULTS_REPORT.md    # Phase 2 results summary
â”‚   â””â”€â”€ README.md                   # Analysis documentation
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ experiments.py              # Phase 1 & 2 configurations
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ data_models.py              # Data structures
â”‚   â”œâ”€â”€ llm_providers.py            # LLM API interfaces
â”‚   â”œâ”€â”€ persistence.py              # Result storage
â”‚   â”œâ”€â”€ data_manager.py             # Data utilities
â”‚   â”œâ”€â”€ utils.py                    # Helper functions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ phase1/                     # Phase 1 experimental results
â”‚   â”œâ”€â”€ phase2/                     # Phase 2 experimental results
â”‚   â””â”€â”€ README.md                   # Data documentation
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ experiment_runner.py        # Core experiment logic
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ old/                            # Original friend's experiments
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_experiment_runner.py   # Engine tests
â”‚   â”œâ”€â”€ test_core_modules.py        # Core module tests
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ main_phase1.py                  # Run Phase 1 experiment
â”œâ”€â”€ main_phase2.py                  # Run Phase 2 experiment
â”œâ”€â”€ pyproject.toml                  # Dependencies and config
â””â”€â”€ README.md                       # Project documentation

Analysis Commands:
  python analysis/generate_phase1_reports.py [--summary-only]
  python analysis/generate_phase2_reports.py [--summary-only]
```
