"""
Core data models for the thinking transplant experiment.

This module defines type-safe data structures using Pydantic for:
- Math problems with ground truth answers
- Trial results from individual experiments
- Experiment configurations
- Error handling and validation

Each data model has a focused purpose and minimal interface for
clear separation of concerns.
"""

from pydantic import BaseModel, Field, field_validator, ConfigDict
from typing import Optional, List, Dict, Any, Literal
from datetime import datetime
from enum import Enum


class ExperimentPhase(str, Enum):
    """Enumeration of experiment phases."""
    PHASE_1 = "phase_1"
    PHASE_2 = "phase_2" 
    PHASE_3 = "phase_3"


class ConditionType(str, Enum):
    """Enumeration of experimental conditions."""
    BASELINE = "baseline"
    THINK_ABOUT_SOLUTION = "think_about_solution"
    MEMORIZED = "memorized"
    COMPLEX_STORY = "complex_story"
    PYTHON_PROGRAM = "python_program"
    GENERATE_RANDOM_NUMBERS = "generate_random_numbers"
    WITH_TRANSPLANTED_NUMBERS = "with_transplanted_numbers"
    BASELINE_NO_NUMBERS = "baseline_no_numbers"
    WITH_RANDOM_NUMBERS = "with_random_numbers"  # Completely random numbers baseline


class MathProblem(BaseModel):
    """
    Represents a mathematical problem with its expected answer.
    
    Encapsulates the problem statement and ground truth for accuracy measurement.
    """
    id: str = Field(..., description="Unique identifier for the problem")
    question: str = Field(..., description="The mathematical problem statement")
    expected_answer: str = Field(..., description="Ground truth answer as string for precision")
    
    @field_validator('question')
    @classmethod
    def question_not_empty(cls, v):
        if not v.strip():
            raise ValueError("Question cannot be empty")
        return v.strip()

    @field_validator('expected_answer')
    @classmethod
    def answer_not_empty(cls, v):
        if not v.strip():
            raise ValueError("Expected answer cannot be empty")
        return v.strip()


class TrialResult(BaseModel):
    """
    Result of a single experimental trial.
    
    Encapsulates all data from one model's attempt at one problem under one condition.
    Follows fail-fast principle with comprehensive error tracking.
    """
    # Core identifiers
    trial_id: str = Field(..., description="Unique identifier for this trial")
    model_name: str = Field(..., description="Name of the model tested")
    condition: ConditionType = Field(..., description="Experimental condition")
    phase: ExperimentPhase = Field(..., description="Which experiment phase")
    
    # Problem and response data
    problem: MathProblem = Field(..., description="The problem that was tested")
    full_response: Optional[str] = Field(None, description="Complete model response")
    first_answer: Optional[str] = Field(None, description="Content of <answer1> tag")
    math_answer: Optional[str] = Field(None, description="Content of <answer2> tag or final answer")
    
    # Experimental data
    generated_numbers: Optional[List[int]] = Field(None, description="Numbers generated by AI")
    transplanted_numbers: Optional[List[int]] = Field(None, description="Numbers provided to AI")
    
    # Accuracy metrics
    digits_correct: Optional[int] = Field(None, description="Number of correct digits from start")
    
    # Error handling
    error: Optional[str] = Field(None, description="Error message if trial failed")
    
    # Metadata
    timestamp: datetime = Field(default_factory=datetime.now, description="When trial was run")
    duration_seconds: Optional[float] = Field(None, description="How long the trial took")
    
    model_config = ConfigDict(
        use_enum_values=True,
        validate_assignment=True
    )


class ExperimentConfig(BaseModel):
    """
    Configuration for a complete experiment phase.
    
    Defines what problems, conditions, and models to test.
    Supports dependency injection by accepting provider configurations.
    """
    name: str = Field(..., description="Human-readable name for this experiment")
    phase: ExperimentPhase = Field(..., description="Which experiment phase this is")
    description: str = Field(..., description="Description of what this experiment tests")
    
    # Experimental design
    conditions: List[ConditionType] = Field(..., description="Conditions to test")
    math_problems: List[MathProblem] = Field(..., description="Problems to solve")
    model_names: List[str] = Field(..., description="Models to test")
    
    # Execution parameters - NO DEFAULTS to force explicit configuration
    iterations_per_condition: int = Field(..., description="Replications per condition")
    max_retries: int = Field(..., description="Max retries for failed API calls")
    timeout_seconds: float = Field(..., description="Timeout for each API call")
    
    # Output configuration
    output_filename_template: str = Field(..., description="Template for output filename")
    
    @field_validator('conditions')
    @classmethod
    def conditions_not_empty(cls, v):
        if not v:
            raise ValueError("Must specify at least one condition")
        return v

    @field_validator('math_problems')
    @classmethod
    def problems_not_empty(cls, v):
        if not v:
            raise ValueError("Must specify at least one math problem")
        return v

    @field_validator('model_names')
    @classmethod
    def models_not_empty(cls, v):
        if not v:
            raise ValueError("Must specify at least one model")
        return v


class ExperimentResults(BaseModel):
    """
    Complete results from an experiment phase.
    
    Aggregates all trial results with summary statistics.
    """
    config: ExperimentConfig = Field(..., description="Configuration used for this experiment")
    trials: List[TrialResult] = Field(default_factory=list, description="All trial results")
    
    # Summary statistics - NO DEFAULTS to ensure explicit calculation
    total_trials: int = Field(..., description="Total number of trials run")
    successful_trials: int = Field(..., description="Number of successful trials")
    failed_trials: int = Field(..., description="Number of failed trials")
    
    # Timing
    start_time: Optional[datetime] = Field(None, description="When experiment started")
    end_time: Optional[datetime] = Field(None, description="When experiment ended")
    total_duration_seconds: Optional[float] = Field(None, description="Total experiment duration")
    
    def add_trial(self, trial: TrialResult) -> None:
        """Add a trial result and update summary statistics."""
        self.trials.append(trial)
        self.total_trials += 1
        
        if trial.error is None:
            self.successful_trials += 1
        else:
            self.failed_trials += 1
    
    def get_accuracy_by_condition(self) -> Dict[str, float]:
        """Calculate mean accuracy (digits correct) by condition."""
        from collections import defaultdict
        
        condition_stats = defaultdict(list)
        
        for trial in self.trials:
            if trial.digits_correct is not None:
                # Handle both enum and string values
                condition_key = trial.condition.value if hasattr(trial.condition, 'value') else trial.condition
                condition_stats[condition_key].append(trial.digits_correct)
        
        return {
            condition: sum(scores) / len(scores) if scores else 0.0
            for condition, scores in condition_stats.items()
        }


class ProviderConfig(BaseModel):
    """
    Configuration for an LLM provider.
    
    Supports dependency injection by encapsulating provider-specific settings.
    """
    provider_type: Literal["openai", "anthropic"] = Field(..., description="Type of provider")
    model_name: str = Field(..., description="Specific model to use")
    api_key_env_var: str = Field(..., description="Environment variable for API key")
    max_tokens: int = Field(default=2000, description="Maximum tokens in response")
    temperature: float = Field(default=0.7, description="Sampling temperature")
    
    @field_validator('temperature')
    @classmethod
    def temperature_valid_range(cls, v):
        if not 0.0 <= v <= 2.0:
            raise ValueError("Temperature must be between 0.0 and 2.0")
        return v
