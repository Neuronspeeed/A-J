"""
Main entry point for Phase 2 of the thinking transplant experiment.

This script tests the core hypothesis: "Can you transplant thinking and have it still work?"

It automatically loads random numbers generated by AI in Phase 1 and tests whether
providing those numbers to the same models improves their performance on new problems.

Usage:
    uv run python main_phase2.py
"""

import asyncio
import sys
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
from pathlib import Path

# Add project root to path for imports
sys.path.append(str(Path(__file__).parent))

from core.llm_providers import create_provider, LLMProviderError
from core.persistence import CsvResultWriter, find_latest_results_file
from engine.experiment_runner import ExperimentRunner
from config.experiments import PHASE2_CONFIG, get_provider_config


async def main():
    """
    Main function for Phase 2 experiment.
    
    This function:
    1. Validates that Phase 1 results exist
    2. Creates dependencies and runs the transplant experiment
    3. Compares baseline vs transplanted number performance
    4. Reports whether the transplant hypothesis is confirmed
    """
    print("Phase 2: Can you transplant thinking and have it still work?")
    print("=" * 70)
    
    # Check for Phase 1 results
    phase1_file = find_latest_results_file("phase1_results_*.csv")
    if not phase1_file:
        print("âŒ No Phase 1 results found!")
        print("Please run Phase 1 first:")
        print("  uv run python main_phase1.py")
        return 1
    
    print(f"ğŸ“ Using Phase 1 results from: {phase1_file}")
    
    # Validate API keys
    try:
        test_config = get_provider_config("gpt-4o")
        test_provider = create_provider(test_config)
        print("âœ… API keys validated")
    except Exception as e:
        print(f"âŒ API key validation failed: {e}")
        return 1
    
    # Create output filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = PHASE2_CONFIG.output_filename_template.format(timestamp=timestamp)
    
    print(f"ğŸ“ Results will be saved to: {output_filename}")
    print(f"ğŸ§ª Testing {len(PHASE2_CONFIG.model_names)} models")
    print(f"ğŸ“‹ Testing {len(PHASE2_CONFIG.conditions)} conditions")
    print(f"ğŸ”¢ Testing {len(PHASE2_CONFIG.math_problems)} math problems")
    
    total_trials = (
        len(PHASE2_CONFIG.model_names) * 
        len(PHASE2_CONFIG.conditions) * 
        len(PHASE2_CONFIG.math_problems) *
        PHASE2_CONFIG.iterations_per_condition
    )
    print(f"ğŸ“Š Total trials: {total_trials}")
    
    # Create result writer
    writer = CsvResultWriter(output_filename)
    
    all_results = []
    
    try:
        # Run experiment for each model
        for model_name in PHASE2_CONFIG.model_names:
            print(f"\nğŸ¤– Testing model: {model_name}")
            
            try:
                # Create provider for this model
                provider_config = get_provider_config(model_name)
                provider = create_provider(provider_config)
                
                # Create experiment runner
                runner = ExperimentRunner(provider=provider, writer=writer)
                
                # Create config for this specific model
                model_config = PHASE2_CONFIG.model_copy(deep=True)
                model_config.model_names = [model_name]
                
                # Run experiment
                results = await runner.run_experiment(model_config)
                all_results.append(results)
                
                print(f"âœ… Completed {model_name}: {results.successful_trials}/{results.total_trials} successful")
                
            except LLMProviderError as e:
                print(f"âŒ Provider error for {model_name}: {e}")
                continue
            except Exception as e:
                print(f"âŒ Unexpected error for {model_name}: {e}")
                continue
    
    finally:
        # Finalize output file
        final_filename = writer.finalize()
        print(f"\nğŸ“ Results saved to: {final_filename}")
    
    # Analyze transplant effect
    if all_results:
        print(f"\n{'='*70}")
        print("PHASE 2 ANALYSIS: TRANSPLANT EFFECT")
        print(f"{'='*70}")
        
        # Aggregate results across all models
        baseline_accuracies = []
        transplant_accuracies = []
        
        for results in all_results:
            accuracy_by_condition = results.get_accuracy_by_condition()
            
            baseline_acc = accuracy_by_condition.get('baseline_no_numbers')
            transplant_acc = accuracy_by_condition.get('with_transplanted_numbers')
            
            if baseline_acc is not None:
                baseline_accuracies.append(baseline_acc)
            if transplant_acc is not None:
                transplant_accuracies.append(transplant_acc)
        
        if baseline_accuracies and transplant_accuracies:
            baseline_mean = sum(baseline_accuracies) / len(baseline_accuracies)
            transplant_mean = sum(transplant_accuracies) / len(transplant_accuracies)
            
            print(f"Baseline (no numbers): {baseline_mean:.2f} average digits correct")
            print(f"With transplanted numbers: {transplant_mean:.2f} average digits correct")
            
            difference = transplant_mean - baseline_mean
            percent_change = (difference / baseline_mean * 100) if baseline_mean > 0 else 0
            
            print(f"\nTransplant Effect:")
            print(f"  Difference: {difference:+.2f} digits")
            print(f"  Percent change: {percent_change:+.1f}%")
            
            if transplant_mean > baseline_mean:
                print(f"  âœ… TRANSPLANT SUCCESSFUL: Random numbers improved performance!")
                print(f"     This suggests AI 'thinking' can be transmitted through numbers.")
            elif transplant_mean < baseline_mean:
                print(f"  âŒ TRANSPLANT FAILED: Random numbers hurt performance")
                print(f"     The numbers may have been distracting rather than helpful.")
            else:
                print(f"  â– NO EFFECT: Random numbers had no impact")
                print(f"     The transplant hypothesis is not supported.")
            
            # Statistical significance note
            print(f"\nNote: For statistical significance, consider running with more iterations")
            print(f"or analyzing individual model results for consistency.")
        
        # Show per-model results
        print(f"\nPer-Model Results:")
        for i, results in enumerate(all_results):
            model_name = PHASE2_CONFIG.model_names[i]
            accuracy_by_condition = results.get_accuracy_by_condition()
            
            baseline = accuracy_by_condition.get('baseline_no_numbers', 0)
            transplant = accuracy_by_condition.get('with_transplanted_numbers', 0)
            
            effect = "âœ… HELPED" if transplant > baseline else "âŒ HURT" if transplant < baseline else "â– NO EFFECT"
            print(f"  {model_name}: {baseline:.2f} â†’ {transplant:.2f} ({effect})")
        
        print(f"\nğŸ¯ Phase 2 complete!")
        print(f"   Consider running Phase 3 to test cross-problem transplantation.")
        
    return 0


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâš ï¸  Experiment interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        sys.exit(1)
