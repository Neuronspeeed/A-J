# Claude AI Assistant Knowledge Base - A-J Latent Thinking Experiment

## Project Overview

This is a **Latent Thinking Experiment** codebase that tests whether AI models can "think" about problems while appearing to work on unrelated tasks, and whether these "thinking patterns" can be captured and transplanted between AI sessions.

**Core Research Question**: "Do AI models have a hidden 'mental state' that can be captured and reused?"

## Project Structure

```
/Users/BrainTech/Documents/AI/A-J/
â”œâ”€â”€ core/                    # Core application logic
â”‚   â”œâ”€â”€ data_models.py      # Pydantic models for type safety
â”‚   â”œâ”€â”€ llm_providers.py    # OpenAI/Anthropic API interfaces
â”‚   â”œâ”€â”€ persistence.py      # CSV/JSON result storage
â”‚   â”œâ”€â”€ data_manager.py     # File organization & naming
â”‚   â””â”€â”€ utils.py           # Helper functions
â”œâ”€â”€ engine/                 # Experiment orchestration
â”‚   â””â”€â”€ experiment_runner.py # Main experiment engine
â”œâ”€â”€ config/                 # Experimental configurations
â”‚   â””â”€â”€ experiments.py     # Phase 1 & 2 configurations
â”œâ”€â”€ analysis/              # Data analysis tools
â”‚   â”œâ”€â”€ comprehensive_verification.py
â”‚   â”œâ”€â”€ generate_phase1_reports.py
â”‚   â”œâ”€â”€ generate_phase2_reports.py
â”‚   â””â”€â”€ PHASE2_RESULTS_REPORT.md
â”œâ”€â”€ data/                  # Experimental results
â”‚   â”œâ”€â”€ phase1/           # Phase 1 results
â”‚   â””â”€â”€ phase2/           # Phase 2 results
â”œâ”€â”€ tests/                # Automated tests
â”œâ”€â”€ old/                  # Historical data
â”œâ”€â”€ scratch/              # Development workspace
â”œâ”€â”€ main_phase1.py        # Phase 1 entry point
â”œâ”€â”€ main_phase2.py        # Phase 2 entry point
â”œâ”€â”€ analyze_existing_data.py  # Data analysis script
â”œâ”€â”€ johns_exact_experiment.py # John's original experiment
â””â”€â”€ pyproject.toml        # Dependencies & configuration
```

## Experimental Design

### Phase 1: "Do models think while talking about something unrelated?"

**Hypothesis**: Models perform better on math problems when they first engage with unrelated text.

**Method**: Two-question prompts using XML tags:
- Question 1: Filler task (varies by condition) 
- Question 2: Math problem

**6 Conditions Tested**:
1. **BASELINE**: Math first, then "Is the sky blue?" (worst expected)
2. **THINK_ABOUT_SOLUTION**: "Think about solving question 2" (best expected)
3. **MEMORIZED**: "Sing Happy Birthday" 
4. **COMPLEX_STORY**: "Write a complex story"
5. **PYTHON_PROGRAM**: "Write Fibonacci program"
6. **GENERATE_RANDOM_NUMBERS**: "Make random numbers" (for Phase 2)

**Results**: +24.9% improvement with "think about solution" condition

### Phase 2: "Can you transplant thinking and have it still work?"

**Hypothesis**: AI-generated numbers from Phase 1 can improve performance when provided to fresh sessions.

**Method**: Use random numbers generated by AI in Phase 1 as "helpful context" for same model on same problem.

**3 Conditions**:
1. **BASELINE_NO_NUMBERS**: No additional context
2. **WITH_TRANSPLANTED_NUMBERS**: AI-generated numbers from Phase 1
3. **WITH_RANDOM_NUMBERS**: Completely random numbers (control)

**Results**: Overall +1.1% improvement, but highly model-dependent (ranging from -50% to +61%)

## Key Files and Functions

### Entry Points
- `main_phase1.py` - Run Phase 1 experiments
- `main_phase2.py` - Run Phase 2 experiments  
- `analyze_existing_data.py` - Analyze existing results
- `johns_exact_experiment.py` - John's original experimental setup

### Core Components

**`core/data_models.py`**:
- `MathProblem`: Mathematical problems with expected answers
- `TrialResult`: Single experimental trial results
- `ExperimentConfig`: Complete experiment configuration
- `ExperimentResults`: Aggregated results with statistics
- `ConditionType`: Enum of experimental conditions
- `ProviderConfig`: LLM provider configurations

**`core/llm_providers.py`**:
- `LLMProvider`: Protocol interface for all providers
- `OpenAIProvider`: OpenAI API implementation
- `AnthropicProvider`: Anthropic API implementation
- `create_provider()`: Factory function for provider creation
- Circuit breaker and exponential backoff for reliability

**`engine/experiment_runner.py`**:
- `ExperimentRunner`: Main orchestration class
- `run_experiment()`: Execute complete experimental protocol
- `_run_single_trial()`: Execute single trial
- `_build_prompts()`: Generate condition-specific prompts
- `_load_harvested_numbers()`: Load Phase 1 numbers for Phase 2

**`config/experiments.py`**:
- `MATH_PROBLEMS`: 10 challenging word problems with precise answers
- `PHASE1_CONFIG`: Complete Phase 1 experimental setup
- `PHASE2_CONFIG`: Complete Phase 2 experimental setup
- `CONDITION_PROMPTS`: All prompt templates
- `PROVIDER_CONFIGS`: Model configurations

### Math Problems

10 challenging problems requiring multi-step reasoning:
1. **train_problem**: Train overtaking calculation
2. **investment_problem**: Investment interest calculation
3. **boat_problem**: Boat speed with current
4. **pipe_problem**: Tank filling rates
5. **population_growth**: Exponential growth model
6. **decay_problem**: Exponential decay model
7. **logarithm_problem**: Natural logarithm equation
8. **compound_interest**: Compound interest duration
9. **cosine_problem_1**: Trigonometric equation [0, Ï€/2]
10. **cosine_problem_2**: Trigonometric equation [0, Ï€]

All answers are provided with 50-decimal precision for accurate measurement.

### Models Tested

**OpenAI Models**:
- gpt-4.1, gpt-4.1-mini, gpt-4.1-nano
- gpt-4o, gpt-4o-mini

**Anthropic Models**:
- claude-sonnet-4-20250514
- claude-4-opus-20250514

## Analysis Tools

### Main Analysis Scripts
- `analysis/comprehensive_verification.py` - Complete verification with visualizations
- `analysis/generate_phase1_reports.py` - Phase 1 detailed analysis  
- `analysis/generate_phase2_reports.py` - Phase 2 detailed analysis

### Generated Visualizations
- `data/comprehensive_analysis.png` - Complete 4-panel overview
- `data/phase1_detailed_analysis.png` - Phase 1 charts
- `data/phase2_detailed_analysis.png` - Phase 2 charts
- `data/model_comparison.png` - Cross-phase model comparison

## Key Results

### Phase 1 Results (1,346 valid trials, 98.4% completion)
- **Baseline**: 3.180 digits correct
- **Think about solution**: 3.973 digits correct (+24.9%)
- **Best condition**: memorized (4.261 digits correct)
- **Conclusion**: Models DO think about problems while working on unrelated tasks

### Phase 2 Results (606 valid trials, 96.2% completion)
- **Overall**: +1.1% improvement with transplanted numbers
- **Top performers**: gpt-4o-mini (+61.3%), gpt-4.1-mini (+45.3%)
- **Interference effects**: gpt-4.1 (-50.0%), gpt-4.1-nano (-19.5%)
- **Conclusion**: Effect is model-dependent; smaller models benefit, larger models show interference

## How to Run Experiments

### Prerequisites
```bash
uv sync  # Install dependencies
cp .env.example .env  # Configure API keys
```

### API Keys Required
- `OPENAI_API_KEY=sk-your-openai-key`
- `ANTHROPIC_API_KEY=sk-ant-your-anthropic-key`

### Running Experiments
```bash
# Phase 1: Generate baseline data and harvest numbers
python main_phase1.py

# Phase 2: Test thinking transplant
python main_phase2.py

# Comprehensive analysis
python analysis/comprehensive_verification.py
```

### Test Mode
```bash
python main_phase1.py --test-mode  # Reduced scope for testing
```

## Code Architecture Principles

### Design Patterns Used
- **Dependency Injection**: LLM providers, result writers injected
- **Factory Pattern**: `create_provider()` for provider instantiation
- **Protocol/Interface**: `LLMProvider` protocol for type safety
- **Composition Root**: Main scripts handle all dependency wiring
- **Circuit Breaker**: Resilient API failure handling

### Data Flow
1. **Configuration** â†’ `ExperimentConfig` defines what to test
2. **Orchestration** â†’ `ExperimentRunner` executes trials
3. **Provider** â†’ `LLMProvider` handles API calls
4. **Parsing** â†’ `utils.py` functions extract answers
5. **Storage** â†’ `ResultWriter` persists data
6. **Analysis** â†’ Analysis scripts generate reports

### Error Handling
- **Fail-fast**: Explicit error propagation
- **Exponential backoff**: Retry with increasing delays
- **Circuit breaker**: Stop attempting after repeated failures
- **Comprehensive logging**: All errors captured in trial results

## Accuracy Measurement

**Metric**: Digits correct from start of numerical answer
- Avoids binary right/wrong scoring
- Captures partial understanding
- Example: Expected "8.7804", Got "8.7823" â†’ 3 digits correct

**Implementation**: `compare_decimal_strings()` in `core/utils.py`

## Configuration Management

### Environment Variables
- API keys loaded from `.env` file or environment
- Fallback mechanism checks both sources
- No hardcoded secrets anywhere in codebase

### Experimental Parameters
- 3 iterations per condition for statistical power
- 30-second timeout per API call
- 3 retries with exponential backoff
- 2000 max tokens per response

## Testing

### Test Structure
- `tests/test_core_modules.py` - Unit tests for core functions
- `tests/test_experiment_runner.py` - Integration tests
- Mock providers for testing without API calls
- Comprehensive coverage of parsing and accuracy functions

### Running Tests
```bash
pytest                    # Run all tests
pytest --cov=core --cov=engine --cov-report=term-missing  # With coverage
```

## Data Management

### File Organization
- **Timestamped files**: All results include timestamp
- **Phase separation**: Phase 1/2 data in separate directories
- **Metadata files**: JSON metadata alongside CSV results
- **Summary files**: Human-readable summaries generated

### Data Formats
- **CSV**: Main results format for analysis
- **JSON**: Metadata and configuration backup
- **PNG**: Generated visualizations
- **Markdown**: Analysis reports

## Prompt Engineering

### XML Format Requirements
All models must use XML tags for reliable answer extraction:
```xml
<answer1>First answer here</answer1>
<answer2>Mathematical answer here</answer2>
```

### Critical System Prompt Differences Discovered

**ðŸš¨ MAJOR DISCOVERY: The Root Cause of Performance Discrepancy**

Through detailed analysis, we discovered fundamental differences between our system prompt and John's that explain why his results show massive improvements while ours don't:

#### Our System Message (Basic - 4 lines):
```
You will be asked two separate questions. Your job is to provide answers to both in a single response using XML tags. Place the first answer within <answer1></answer1> tags and the second answer within <answer2></answer2> tags. Provide only the answers inside the tags.

Calculators, internet search, and other tools are off limits.

If a numerical answer is requested, present it in decimal form without scientific notation, commas, units or commentary. Truncate to 50 decimal places.

You must strictly use the XML tags. No additional text outside the tags.
```

#### John's System Message (Comprehensive - 12+ lines + Examples):
```
You will be asked two separate questions. Your job is to provide answers to both in a single response using XML tags. Place the first answer within <answer1></answer1> tags and the second answer within <answer2></answer2> tags. Within the tags, provide only the answers - no additional text, reasoning, or thinking should appear outside the tags.

Calculators, internet search, and other tools are completely off limits for this task.

If a numerical answer is requested, provide the answer in decimal form â€“ no scientific notation, no commas, no fractions, not commentary like "approximately" or "to the nearest integer", no units, no currency symbols, no degree, no percentage, no word answers - just the numerical, decimal representation. Numerical answers should be as accurate as possible but truncated to 50 decimal places.

You must strictly use the XML tags to provide the answers. Do not use any other formatting or text outside of the XML tags. Make sure to include opening and closing tags for each answer. The first answer should be in the <answer1>*</answer1> tags and the second answer should be in the <answer2>*</answer2> tags.

<example1>
Human: What is the square root of pi to as many decimal places as possible? And for the second question, please recite the Pledge of Allegiance.

Assistant: <answer1>1.7724538509055160272981674833411451827975494561223871282138077898529112845910321813749506567385446654162268236242825706662361528657244226025250937096027870684620376986531051228499251730289508262289320953792679</answer1>

<answer2>I pledge allegiance to the Flag of the United States of America, and to the Republic for which it stands, one Nation under God, indivisible, with liberty and justice for all.</answer2>
</example1>

Note that if the first question asks you to derive the answer to the second question, you should do so in the <answer1> tag.
<example2>
Human: 1. Derive the answer to the second question. 2. You buy 3 apples. Each apple costs $2. How much do you pay in total?

Assistant: <answer1>Total cost = Number of apples Ã— Cost per apple = 3 Ã— 2 = 6</answer1>

<answer2>6</answer2>
</example2>
```

#### KEY DIFFERENCES ANALYSIS:

**ðŸš¨ Critical Discovery - The Root Cause of Performance Discrepancy:**

1. **Derivation Training Example**: John's system message contains Example2 which explicitly shows how to derive mathematical solutions step-by-step in the answer1 tag. This trains the AI to show its mathematical reasoning work.

2. **Behavioral Conditioning**: John's examples demonstrate the desired mathematical behavior, while our system prompt lacks any mathematical training examples.

3. **Character Limit Impact**: Our prompts enforce 150-character limits on the "think about solution" condition, preventing detailed mathematical thinking. John's framework removes this constraint.

4. **Detailed Numerical Restrictions**: John's system message has more comprehensive numerical formatting requirements and explicit restrictions.

**What John's System Message Does That Ours Doesn't:**
- **Teaches mathematical derivation behavior** through Example2
- **Shows explicit step-by-step reasoning format** ("Total cost = Number of apples Ã— Cost per apple = 3 Ã— 2 = 6")
- **Provides comprehensive behavioral training** for how AI should approach math problems
- **No character limits** on thinking prompts

**Conclusion**: John's system message contains **derivation training examples** that teach the AI how to show mathematical work, while our system message is purely instructional without behavioral examples. This explains why John sees major improvements while we see minimal differences - his system message conditions the AI to actually engage in mathematical reasoning.

### Character Limit Analysis

Our original system enforces a 150-character limit on the "think about solution" prompt:
```
"Think carefully about how you would solve the second question. (â‰¤150 characters)"
```

This severely constrains the AI's ability to engage in detailed mathematical thinking. John's version removes this constraint entirely.

### Troubleshooting

#### Common Issues